@inbook{Shi2022,
	title        = {Heterogeneous Graph Neural Networks},
	author       = {Chuan Shi},
	year         = 2022,
	month        = 1,
	booktitle    = {Graph Neural Networks: Foundations, Frontiers, and Applications},
	publisher    = {Springer Nature},
	pages        = {351--369},
	doi          = {10.1007/978-981-16-6054-2_16},
	isbn         = 9789811660542,
	abstract     = {Heterogeneous graphs (HGs) also called heterogeneous information networks (HINs) have become ubiquitous in real-world scenarios. Recently, employing graph neural networks (GNNs) to heterogeneous graphs, known as heterogeneous graph neural networks (HGNNs) which aim to learn embedding in low-dimensional space while preserving heterogeneous structure and semantic for downstream tasks, has drawn considerable attention. This chapter will first give a brief review of the recent development on HG embedding, then introduce typical methods from the perspective of shallow and deep models, especially HGNNs. Finally, it will point out future research directions for HGNNs.},
	editor       = {Lingfei Wu and Peng Cui and Jian Pei and Liang Zhao}
}
@inproceedings{Hu2020,
	title        = {Heterogeneous graph transformer},
	author       = {Ziniu Hu and Yuxiao Dong and Kuansan Wang and Yizhou Sun},
	year         = 2020,
	booktitle    = {Proceedings of the web conference 2020},
	pages        = {2704--2710},
	doi          = {10.1145/3366423.3380027}
}
@article{Yang2020,
	title        = {Heterogeneous Network Representation Learning: A Unified Framework with Survey and Benchmark},
	author       = {Carl Yang and Yuxin Xiao and Yu Zhang and Yizhou Sun and Jiawei Han},
	year         = 2020,
	journal      = {IEEE Transactions on Knowledge and Data Engineering},
	publisher    = {IEEE Computer Society},
	volume       = 34,
	pages        = {4854--4873},
	doi          = {10.1109/TKDE.2020.3045924},
	issn         = 15582191,
	abstract     = {Since real-world objects and their interactions are often multi-modal and multi-typed, heterogeneous networks have been widely used as a more powerful, realistic, and generic superclass of traditional homogeneous networks (graphs). Meanwhile, representation learning (a.k.a. embedding) has recently been intensively studied and shown effective for various network mining and analytical tasks. In this work, we aim to provide a unified framework to deeply summarize and evaluate existing research on heterogeneous network embedding (HNE), which includes but goes beyond a normal survey. Since there has already been a broad body of HNE algorithms, as the first contribution of this work, we provide a generic paradigm for the systematic categorization and analysis over the merits of various existing HNE algorithms. Moreover, existing HNE algorithms, though mostly claimed generic, are often evaluated on different datasets. As the second contribution, we create four benchmark datasets with various properties regarding scale, structure, attribute/label availability, and etc. from different sources, towards handy and fair evaluations of HNE algorithms. As the third contribution, we carefully refactor and amend the implementations and create friendly interfaces for eleven popular HNE algorithms, and provide all-around comparisons among them over multiple tasks and experimental settings.},
	issue        = 10,
	keywords     = {Analytical models,Benchmark testing,Heterogeneous networks,Mathematical model,Systematics,Task analysis,Toy manufacturing industry,benchmark,heterogeneous network,representation learning,survey}
}
@article{Wang2019,
	title        = {Heterogeneous Graph Attention Network},
	author       = {Xiao Wang and Houye Ji and Peng Cui and P. Yu and Chuan Shi and Bai Wang and Yanfang Ye},
	year         = 2019,
	month        = 5,
	journal      = {The World Wide Web Conference},
	publisher    = {Association for Computing Machinery, Inc},
	pages        = {2022--2032},
	doi          = {10.1145/3308558.3313562},
	isbn         = 9781450366748,
	abstract     = {Graph neural network, as a powerful graph representation technique based on deep learning, has shown superior performance and attracted considerable research interest. However, it has not been fully considered in graph neural network for heterogeneous graph which contains different types of nodes and links. The heterogeneity and rich semantic information bring great challenges for designing a graph neural network for heterogeneous graph. Recently, one of the most exciting advancements in deep learning is the attention mechanism, whose great potential has been well demonstrated in various areas. In this paper, we first propose a novel heterogeneous graph neural network based on the hierarchical attention, including node-level and semantic-level attentions. Specifically, the node-level attention aims to learn the importance between a node and its meta-path based neighbors, while the semantic-level attention is able to learn the importance of different meta-paths. With the learned importance from both node-level and semantic-level attention, the importance of node and meta-path can be fully considered. Then the proposed model can generate node embedding by aggregating features from meta-path based neighbors in a hierarchical manner. Extensive experimental results on three real-world heterogeneous graphs not only show the superior performance of our proposed model over the state-of-the-arts, but also demonstrate its potentially good interpretability for graph analysis.},
	keywords     = {Graph Analysis,Neural Network,Social Network}
}
@inproceedings{Lv2021,
	title        = {Are we really making much progress?: Revisiting, benchmarking and refining heterogeneous graph neural networks},
	author       = {Qingsong Lv and Ming Ding and Qiang Liu and Yuxiang Chen and Wenzheng Feng and Siming He and Chang Zhou and Jianguo Jiang and Yuxiao Dong and Jie Tang},
	year         = 2021,
	booktitle    = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	doi          = {10.1145/3447548.3467350},
	abstract     = {Heterogeneous graph neural networks (HGNNs) have been blossoming in recent years, but the unique data processing and evaluation setups used by each work obstruct a full understanding of their advancements. In this work, we present a systematical reproduction of 12 recent HGNNs by using their official codes, datasets, settings, and hyperparameters, revealing surprising findings about the progress of HGNNs. We find that the simple homogeneous GNNs, e.g., GCN and GAT, are largely underestimated due to improper settings. GAT with proper inputs can generally match or outperform all existing HGNNs across various scenarios. To facilitate robust and reproducible HGNN research, we construct the Heterogeneous Graph Benchmark (HGB) , consisting of 11 diverse datasets with three tasks. HGB standardizes the process of heterogeneous graph data splits, feature processing, and performance evaluation. Finally, we introduce a simple but very strong baseline Simple-HGN-which significantly outperforms all previous models on HGB-to accelerate the advancement of HGNNs in the future.}
}
@article{Wang2023,
	title        = {A Survey on Heterogeneous Graph Embedding: Methods, Techniques, Applications and Sources},
	author       = {Xiao Wang and Deyu Bo and Chuan Shi and Shaohua Fan and Yanfang Ye and Philip S. Yu},
	year         = 2023,
	journal      = {IEEE Transactions on Big Data},
	volume       = 9,
	doi          = {10.1109/TBDATA.2022.3177455},
	issn         = 23327790,
	abstract     = {Heterogeneous graphs (HGs) also known as heterogeneous information networks have become ubiquitous in real-world scenarios; therefore, HG embedding, which aims to learn representations in a lower-dimension space while preserving the heterogeneous structures and semantics for downstream tasks (e.g., node/graph classification, node clustering, link prediction), has drawn considerable attentions in recent years. In this survey, we perform a comprehensive review of the recent development on HG embedding methods and techniques. We first introduce the basic concepts of HG and discuss the unique challenges brought by the heterogeneity for HG embedding in comparison with homogeneous graph representation learning; and then we systemically survey and categorize the state-of-the-art HG embedding methods based on the information they used in the learning process to address the challenges posed by the HG heterogeneity. In particular, for each representative HG embedding method, we provide detailed introduction and further analyze its pros and cons; meanwhile, we also explore the transformativeness and applicability of different types of HG embedding methods in the real-world industrial environments for the first time. In addition, we further present several widely deployed systems that have demonstrated the success of HG embedding techniques in resolving real-world application problems with broader impacts. To facilitate future research and applications in this area, we also summarize the open-source code, existing graph learning platforms and benchmark datasets. Finally, we explore the additional issues and challenges of HG embedding and forecast the future research directions in this field.},
	issue        = 2
}
@book{Hogan2022,
	title        = {Knowledge Graphs},
	author       = {A Hogan and E Blomqvist and M Cochez and C dâ€™Amato and G de Melo and C Gutierrez and S Kirrane and J E L Gayo and R Navigli and S Neumaier and others},
	year         = 2022,
	publisher    = {Springer International Publishing},
	doi          = {10.1145/3447772},
	isbn         = 9783031019180,
	url          = {https://books.google.pl/books?id=4YdyEAAAQBAJ}
}
@article{Shi2022,
	title        = {Heterogeneous graph neural networks},
	author       = {Chuan Shi},
	year         = 2022,
	journal      = {Graph Neural Networks: Foundations, Frontiers, and Applications},
	publisher    = {Springer},
	pages        = {351--369},
	doi          = {10.1007/978-981-16-6054-2_16}
}
@book{Kejriwal2021,
	title        = {Knowledge graphs: Fundamentals, techniques, and applications},
	author       = {Mayank Kejriwal and Craig A Knoblock and Pedro Szekely},
	year         = 2021,
	publisher    = {MIT Press}
}
@inproceedings{Hagberg2008,
	title        = {Exploring Network Structure, Dynamics, and Function using NetworkX},
	author       = {Aric A Hagberg and Daniel A Schult and Pieter J Swart},
	year         = 2008,
	booktitle    = {Proceedings of the 7th Python in Science Conference},
	pages        = {11--15},
	doi          = {10.25080/TCWV9851},
	city         = {Pasadena, CA USA},
	editor       = {GaÃ«l Varoquaux and Travis Vaught and Jarrod Millman}
}
@misc{Falcon2019,
	title        = {PyTorch Lightning},
	author       = {William Falcon and The PyTorch Lightning team},
	year         = 2019,
	month        = 3,
	doi          = {10.5281/zenodo.3828935},
	url          = {https://github.com/Lightning-AI/lightning}
}
@article{Wojcik2024,
	title        = {An Analysis of Novel Money Laundering Data Using Heterogeneous Graph Isomorphism Networks. FinCEN Files Case Study},
	author       = {Filip WÃ³jcik},
	year         = 2024,
	journal      = {Econometrics. Ekonometria. Advances in Applied Data Analytics},
	volume       = 28,
	pages        = {32--49},
	doi          = {10.15611/eada.2024.2.03},
	issue        = 2
}
@inproceedings{Ansel2024,
	title        = {PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation},
	author       = {Jason Ansel and Edward Yang and Horace He and Natalia Gimelshein and Animesh Jain and Michael Voznesensky and Bin Bao and Peter Bell and David Berard and Evgeni Burovski and Geeta Chauhan and Anjali Chourdia and Will Constable and Alban Desmaison and Zachary DeVito and Elias Ellison and Will Feng and Jiong Gong and Michael Gschwind and Brian Hirsh and Sherlock Huang and Kshiteej Kalambarkar and Laurent Kirsch and Michael Lazos and Mario Lezcano and Yanbo Liang and Jason Liang and Yinghai Lu and C K Luk and Bert Maher and Yunjie Pan and Christian Puhrsch and Matthias Reso and Mark Saroufim and Marcos Yukio Siraichi and Helen Suk and Michael Suo and Phil Tillet and Eikan Wang and Xiaodong Wang and William Wen and Shunting Zhang and Xu Zhao and Keren Zhou and Richard Zou and Ajit Mathews and Gregory Chanan and Peng Wu and Soumith Chintala},
	year         = 2024,
	month        = 4,
	booktitle    = {29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24)},
	publisher    = {ACM},
	doi          = {10.1145/3620665.3640366},
	url          = {https://pytorch.org/assets/pytorch2-2.pdf}
}
@software{Fey_Fast_Graph_Representation_2019,
	title        = {{Fast Graph Representation Learning with PyTorch Geometric}},
	author       = {Fey, Matthias and Lenssen, Jan Eric},
	year         = 2019,
	month        = may,
	doi          = {10.48550/arXiv.1903.02428},
	url          = {https://github.com/pyg-team/pytorch_geometric},
	license      = {MIT}
}
@inproceedings{gilmer2017neural,
	title        = {Neural message passing for quantum chemistry},
	author       = {Gilmer, Justin and Schoenholz, Samuel S and Riley, Patrick F and Vinyals, Oriol and Dahl, George E},
	year         = 2017,
	booktitle    = {International conference on machine learning},
	pages        = {1263--1272},
	doi          = {10.48550/arXiv.1704.01212},
	organization = {PMLR}
}
@article{wang2019dgl,
	title        = {Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks},
	author       = {Minjie Wang and Da Zheng and Zihao Ye and Quan Gan and Mufei Li and Xiang Song and Jinjing Zhou and Chao Ma and Lingfan Yu and Yu Gai and Tianjun Xiao and Tong He and George Karypis and Jinyang Li and Zheng Zhang},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1909.01315},
	doi          = {10.48550/arXiv.1909.01315}
}
@article{noy2019industry,
	title        = {Industry-scale Knowledge Graphs: Lessons and Challenges: Five diverse technology companies show how itâ€™s done},
	author       = {Noy, Natasha and Gao, Yuqing and Jain, Anshu and Narayanan, Anant and Patterson, Alan and Taylor, Jamie},
	year         = 2019,
	journal      = {Queue},
	publisher    = {ACM New York, NY, USA},
	volume       = 17,
	number       = 2,
	pages        = {48--75},
	doi          = {10.1145/3329781.3332266}
}
@inproceedings{chen2018linkedin,
	title        = {How LinkedIn economic graph bonds information and product: applications in LinkedIn salary},
	author       = {Chen, Xi and Liu, Yiqun and Zhang, Liang and Kenthapadi, Krishnaram},
	year         = 2018,
	booktitle    = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
	pages        = {120--129},
	doi          = {10.1145/3219819.3219921}
}
@article{mitra2024knowledge,
	title        = {Knowledge graph driven credit risk assessment for micro, small and medium-sized enterprises},
	author       = {Mitra, Rony and Dongre, Ayush and Dangare, Piyush and Goswami, Adrijit and Tiwari, Manoj Kumar},
	year         = 2024,
	journal      = {International Journal of Production Research},
	publisher    = {Taylor \& Francis},
	volume       = 62,
	number       = 12,
	pages        = {4273--4289},
	doi          = {10.1080/00207543.2023.2257807}
}
@misc{Deng2022,
	title        = {Recommender Systems Based on Graph Embedding Techniques: A Review},
	author       = {Yue Deng},
	year         = 2022,
	journal      = {IEEE Access},
	volume       = 10,
	doi          = {10.1109/ACCESS.2022.3174197},
	issn         = 21693536,
	abstract     = {As a pivotal tool to alleviate the information overload problem, recommender systems aim to predict user's preferred items from millions of candidates by analyzing observed user-item relations. As for alleviating the sparsity and cold start problems encountered by recommender systems, researchers generally resort to employing side information or knowledge in recommendation as a strategy for uncovering hidden (indirect) user-item relations, aiming to enrich observed information (or data) for recommendation. However, in the face of the high complexity and large scale of side information and knowledge, this strategy largely relies for efficient implementation on the scalability of recommendation models. Not until after the prevalence of machine learning did graph embedding techniques be a recent concentration, which can efficiently utilize complex and large-scale data. In light of that, equipping recommender systems with graph embedding techniques has been widely studied these years, appearing to outperform conventional recommendation implemented directly based on graph topological analysis (or resolution). As the focus, this article systematically retrospects graph embedding-based recommendation from embedding techniques for bipartite graphs, general graphs and knowledge graphs, and proposes a general design pipeline of that. In addition, after comparing several representative graph embedding-based recommendation models with the most common-used conventional recommendation models on simulations, this article manifests that the conventional models can still overall outperform the graph embedding-based ones in predicting implicit user-item interactions, revealing the comparative weakness of graph embedding-based recommendation in these tasks. To foster future research, this article proposes constructive suggestions on making a trade-off between graph embedding-based recommendation and conventional recommendation in different tasks, and puts forward some open questions.}
}
@article{Wu2022,
	title        = {Graph Neural Networks in Recommender Systems: A Survey},
	author       = {Shiwen Wu and Fei Sun and Wentao Zhang and Xu Xie and Bin Cui},
	year         = 2022,
	journal      = {ACM Computing Surveys},
	volume       = 55,
	doi          = {10.1145/3535101},
	issn         = 15577341,
	abstract     = {With the explosive growth of online information, recommender systems play a key role to alleviate such information overload. Due to the important application value of recommender systems, there have always been emerging works in this field. In recommender systems, the main challenge is to learn the effective user/item representations from their interactions and side information (if any). Recently, graph neural network (GNN) techniques have been widely utilized in recommender systems since most of the information in recommender systems essentially has graph structure and GNN has superiority in graph representation learning. This article aims to provide a comprehensive review of recent research efforts on GNN-based recommender systems. Specifically, we provide a taxonomy of GNN-based recommendation models according to the types of information used and recommendation tasks. Moreover, we systematically analyze the challenges of applying GNN on different types of data and discuss how existing works in this field address these challenges. Furthermore, we state new perspectives pertaining to the development of this field. We collect the representative papers along with their open-source implementations in https://github.com/wusw14/GNN-in-RS.},
	issue        = 5
}
@article{Jumper2021,
	title        = {Highly accurate protein structure prediction with AlphaFold},
	author       = {John Jumper and Richard Evans and Alexander Pritzel and Tim Green and Michael Figurnov and Olaf Ronneberger and Kathryn Tunyasuvunakool and Russ Bates and Augustin Å½Ã­dek and Anna Potapenko and Alex Bridgland and Clemens Meyer and Simon A.A. Kohl and Andrew J. Ballard and Andrew Cowie and Bernardino Romera-Paredes and Stanislav Nikolov and Rishub Jain and Jonas Adler and Trevor Back and Stig Petersen and David Reiman and Ellen Clancy and Michal Zielinski and Martin Steinegger and Michalina Pacholska and Tamas Berghammer and Sebastian Bodenstein and David Silver and Oriol Vinyals and Andrew W. Senior and Koray Kavukcuoglu and Pushmeet Kohli and Demis Hassabis},
	year         = 2021,
	journal      = {Nature},
	volume       = 596,
	doi          = {10.1038/s41586-021-03819-2},
	issn         = 14764687,
	abstract     = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1â€“4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequenceâ€”the structure prediction component of the â€˜protein folding problemâ€™8â€”has been an important open research problem for more than 50Â years9. Despite recent progress10â€“14, existing methods fall farÂ short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
	issue        = 7873
}
@article{maclean2021knowledge,
	title        = {Knowledge graphs and their applications in drug discovery},
	author       = {MacLean, Finlay},
	year         = 2021,
	journal      = {Expert opinion on drug discovery},
	publisher    = {Taylor \& Francis},
	volume       = 16,
	number       = 9,
	pages        = {1057--1069},
	doi          = {10.1080/17460441.2021.1910673}
}
@article{Johannessen2023,
	title        = {Finding Money Launderers Using Heterogeneous Graph Neural Networks},
	author       = {Fredrik Johannessen and Martin Jullum},
	year         = 2023,
	journal      = {arXiv: 2307.13499},
	doi          = {10.48550/arXiv.2307.13499}
}
